{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13108130,"sourceType":"datasetVersion","datasetId":8293420}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install seqeval --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport csv\nimport math\nimport pandas as pd\nfrom transformers.models import convbert\nfrom typing import Optional\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"seqeval\")\nfrom torch.nn.utils import weight_norm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PSSPDataset(Dataset):\n    def __init__(self, x_path, y_path):\n        self.encodings = torch.load(x_path, map_location='cpu', mmap=True, weights_only=False)\n        self.labels = torch.load(y_path, map_location='cpu', mmap=True, weights_only=False)\n\n    def __getitem__(self, idx):\n        embedding = self.encodings[idx]\n        labels = self.labels[idx]\n        return torch.tensor(embedding), torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    xs, ys = zip(*batch) \n\n    lengths = [x.size(0) for x in xs]\n    max_len = max(lengths)\n\n    x_padded = pad_sequence(xs, batch_first=True)\n    y_padded = pad_sequence(ys, batch_first=True, padding_value=-100)\n\n    attention_mask = torch.zeros(y_padded.shape, dtype=torch.bool)\n    for i, length in enumerate(lengths):\n        attention_mask[i, :length] = True\n\n    src_key_padding_mask = ~attention_mask\n\n    return x_padded, y_padded, src_key_padding_mask","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_dataset = ConcatDataset([\n    PSSPDataset(\n        x_path='/kaggle/input/ankh-embedding-data/train0_ssp8_embeddings.pt',\n        y_path='/kaggle/input/ankh-embedding-data/train0_ssp8_labels.pt'\n    ),\n    PSSPDataset(\n        x_path='/kaggle/input/ankh-embedding-data/train1_ssp8_embeddings.pt',\n        y_path='/kaggle/input/ankh-embedding-data/train1_ssp8_labels.pt'\n    ),\n    PSSPDataset(\n        x_path='/kaggle/input/ankh-embedding-data/train2_ssp8_embeddings.pt',\n        y_path='/kaggle/input/ankh-embedding-data/train2_ssp8_labels.pt'\n    ),\n    PSSPDataset(\n        x_path='/kaggle/input/ankh-embedding-data/train3_ssp8_embeddings.pt',\n        y_path='/kaggle/input/ankh-embedding-data/train3_ssp8_labels.pt'\n    ),\n    PSSPDataset(\n        x_path='/kaggle/input/ankh-embedding-data/train4_ssp8_embeddings.pt',\n        y_path='/kaggle/input/ankh-embedding-data/train4_ssp8_labels.pt'\n    ),\n])\n\nval_dataset = PSSPDataset(\n        x_path='/kaggle/input/ankh-embedding-data/val_ssp8_embeddings.pt',\n        y_path='/kaggle/input/ankh-embedding-data/val_ssp8_labels.pt'\n    )\n\ntest_dataset = PSSPDataset(\n        x_path='/kaggle/input/ankh-embedding-data/test_ssp8_embeddings.pt',\n        y_path='/kaggle/input/ankh-embedding-data/test_ssp8_labels.pt'\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 32\ntrain_loader = DataLoader(training_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_tags = {'B', 'C', 'E', 'G', 'H', 'I', 'S', 'T'}\ntag2id = {'B': 0, 'C': 1, 'I': 2, 'T': 3, 'S': 4, 'E': 5, 'G': 6, 'H': 7}\nid2tag = {0: 'B', 1: 'C', 2: 'I', 3: 'T', 4: 'S', 5: 'E', 6: 'G', 7: 'H'}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, input_dim, num_heads, num_layers=1):\n        super().__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n    def forward(self, x, padding_mask=None):\n        x = x.transpose(0, 1)\n        x = self.transformer_encoder(x, src_key_padding_mask=padding_mask)\n        x = x.transpose(0, 1)\n        return x\n\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, dropout):\n        super().__init__()\n        \n        padding = (kernel_size - 1) * dilation\n\n        self.conv1 = nn.Sequential(\n            weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size=kernel_size, padding=padding,\n                                 stride=stride, dilation=dilation)),\n            Chomp1d(padding),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        self.conv2 = nn.Sequential(\n            weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size=kernel_size, padding=padding,\n                                 stride=stride, dilation=dilation)),\n            Chomp1d(padding),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(res + out)\n\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size, dropout):\n        super().__init__()\n        layers = []\n        num_levels = len(num_channels)\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers.append(\n                TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n                              dilation=dilation_size, dropout=dropout)\n            )\n        self.network = nn.Sequential(*layers)\n        \n\n    def forward(self, x):\n        x = self.network(x)\n        return x\n\nclass BiTCN(nn.Module):\n    def __init__(self, input_dim, num_channels, kernel_size, dropout):\n        super().__init__()\n        self.forward_tcn = TemporalConvNet(input_dim, num_channels, kernel_size, dropout)\n        self.backward_tcn = TemporalConvNet(input_dim, num_channels, kernel_size, dropout)\n\n    def forward(self, x):\n        forward_out = self.forward_tcn(x)\n        backward_out = self.backward_tcn(torch.flip(x, dims=[2]))\n        backward_out = torch.flip(backward_out, dims=[2])\n        out = torch.cat([forward_out, backward_out], dim=1)\n        return out\n\nclass PSSPModel(nn.Module):\n    def __init__(self, num_channels, kernel_size, num_heads, dropout, input_dim=1536, num_layers=1, num_classes=8):\n        super().__init__()\n        self.bitcn = BiTCN(input_dim, num_channels, kernel_size, dropout)\n        self.transformer = Transformer(num_channels[-1]*2, num_heads, num_layers)\n        self.fc = nn.Linear(num_channels[-1]*2, num_classes)\n\n    def forward(self, x, padding_mask=None):\n        x = x.transpose(1, 2)\n        x = self.bitcn(x)\n        x = x.transpose(1, 2)\n        x = self.transformer(x, padding_mask)\n        logits = self.fc(x)\n        return logits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, data_loader, criterion, optimizer, id2tag, device):\n    model.train()\n    loss_total = 0\n\n    all_preds = []\n    all_labels = []\n\n    for x_batch, y_batch, attention_mask in data_loader:\n        x_batch, y_batch, attention_mask = x_batch.to(device), y_batch.to(device), attention_mask.to(device)\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            outputs = model(x_batch, attention_mask)\n            loss = criterion(outputs.view(-1, outputs.shape[-1]), y_batch.view(-1))\n        loss.backward()\n        optimizer.step()\n\n        preds = outputs.argmax(dim=2).cpu().numpy()\n        labels = y_batch.cpu().numpy()\n\n        for i in range(labels.shape[0]):\n            pred_seq, label_seq = [], []\n            for j in range(labels.shape[1]):\n                if labels[i, j] != -100:\n                    pred_seq.append(id2tag[preds[i][j]])\n                    label_seq.append(id2tag[labels[i][j]])\n            all_preds.append(pred_seq)\n            all_labels.append(label_seq)\n\n        loss_total += loss.item()\n\n    avg_loss = loss_total / len(data_loader)\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds)\n    recall = recall_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds)\n\n    return avg_loss, accuracy, f1, precision, recall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eval_model(model, data_loader, criterion, id2tag, device):\n    model.eval()\n    loss_total = 0\n\n    all_preds = []\n    all_labels = []\n\n    for x_batch, y_batch, attention_mask in data_loader:\n        x_batch, y_batch, attention_mask = x_batch.to(device), y_batch.to(device), attention_mask.to(device)\n        with torch.cuda.amp.autocast():\n            outputs = model(x_batch, attention_mask)\n            loss = criterion(outputs.view(-1, outputs.shape[-1]), y_batch.view(-1))\n\n        preds = outputs.argmax(dim=2).cpu().numpy()\n        labels = y_batch.cpu().numpy()\n\n        for i in range(labels.shape[0]):\n            pred_seq, label_seq = [], []\n            for j in range(labels.shape[1]):\n                if labels[i, j] != -100:\n                    pred_seq.append(id2tag[preds[i][j]])\n                    label_seq.append(id2tag[labels[i][j]])\n            all_preds.append(pred_seq)\n            all_labels.append(label_seq)\n\n        loss_total += loss.item()\n\n    avg_loss = loss_total / len(data_loader)\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds)\n    recall = recall_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds)\n\n    return avg_loss, accuracy, f1, precision, recall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(model, data_loader, criterion, device, id2tag, class_names=None):\n    model.eval()\n    loss_total = 0\n\n\n    all_preds_seq, all_labels_seq = [], []\n    all_preds_flat, all_labels_flat = [], []\n\n    for x_batch, y_batch, attention_mask in data_loader:\n        x_batch, y_batch, attention_mask = x_batch.to(device), y_batch.to(device), attention_mask.to(device)\n        with torch.cuda.amp.autocast():\n            outputs = model(x_batch, attention_mask)  # [batch, seq_len, num_classes]\n            loss = criterion(outputs.view(-1, outputs.shape[-1]), y_batch.view(-1))\n        loss_total += loss.item()\n\n        preds = outputs.argmax(dim=2).cpu().numpy()\n        labels = y_batch.cpu().numpy()\n\n        for i in range(labels.shape[0]):\n            pred_seq, label_seq = [], []\n            for j in range(labels.shape[1]):\n                if labels[i, j] != -100:\n                    pred_seq.append(id2tag[preds[i][j]])\n                    label_seq.append(id2tag[labels[i][j]])\n\n                    all_preds_flat.append(preds[i][j])\n                    all_labels_flat.append(labels[i][j])\n\n            all_preds_seq.append(pred_seq)\n            all_labels_seq.append(label_seq)\n\n    avg_loss = loss_total / len(data_loader)\n\n    accuracy = accuracy_score(all_labels_seq, all_preds_seq)\n    precision = precision_score(all_labels_seq, all_preds_seq)\n    recall = recall_score(all_labels_seq, all_preds_seq)\n    f1 = f1_score(all_labels_seq, all_preds_seq)\n\n    if class_names is None:\n        class_names = [id2tag[i] for i in range(len(set(all_labels_flat)))]\n\n    cm = confusion_matrix(all_labels_flat, all_preds_flat)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)\n    plt.title(\"Confusion Matrix Q8\")\n    plt.tight_layout()\n    plt.show()\n    plt.savefig(\"confusion_matrix.png\")\n    plt.close()\n\n    # --- Save predictions to CSV ---\n    with open(\"predictions_and_labels.csv\", mode=\"w\", newline=\"\") as f_csv:\n        writer = csv.writer(f_csv)\n        writer.writerow([\"predictions\", \"labels\"])  # Header\n        for pred_seq, label_seq in zip(all_preds_seq, all_labels_seq):\n            writer.writerow([\" \".join(pred_seq), \" \".join(label_seq)])\n\n    return avg_loss, accuracy, f1, precision, recall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TCN_LEVEL = 3\nNUM_CHANNELS = [256] * TCN_LEVEL\nKERNEL_SIZE = 5\nNUM_HEADS = 8\nDROPOUT = 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = PSSPModel(num_channels=NUM_CHANNELS, \n                  kernel_size=KERNEL_SIZE,\n                  num_heads=NUM_HEADS,\n                 dropout=DROPOUT).to(device)\nprint(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index=-100)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_acc = 0\nbest_epoch = -1\ncheckpoint_path = '/kaggle/working/best_checkpoint.pt'\npatience = 10\nbest_loss = float(\"inf\")\ncounter_loss = 0\nEPOCH = 100\n\nhistory = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n    \"train_acc\": [],\n    \"val_acc\": [],\n    \"train_f1\": [],\n    \"val_f1\": [],\n    \"train_precision\": [],\n    \"val_precision\": [],\n    \"train_recall\": [],\n    \"val_recall\": []\n}\n\nfor epoch in range(EPOCH):\n    # training\n    train_loss, train_acc, train_f1, train_precision, train_recall = train_model(\n        model, train_loader, criterion, optimizer, id2tag, device\n    )\n    print(f'[Epoch {epoch+1}/{EPOCH}] '\n          f'Train Loss: {train_loss:.4f} | '\n          f'Acc: {train_acc:.4f} | '\n          f'F1: {train_f1:.4f} | '\n          f'Precision: {train_precision:.4f} | '\n          f'Recall: {train_recall:.4f}')\n\n    # validation\n    val_loss, val_acc, val_f1, val_precision, val_recall = eval_model(\n        model, val_loader, criterion, id2tag, device\n    )\n    print(f'[Epoch {epoch+1}/{EPOCH}] '\n          f'Val   Loss: {val_loss:.4f} | '\n          f'Acc: {val_acc:.4f} | '\n          f'F1: {val_f1:.4f} | '\n          f'Precision: {val_precision:.4f} | '\n          f'Recall: {val_recall:.4f}')\n\n    # save best model based on acc\n    if val_acc > best_acc:\n        best_acc = val_acc\n        best_epoch = epoch + 1\n        print(f\"✅ Saving best model (Acc = {best_acc:.4f}) at epoch {best_epoch}\")\n        torch.save({\n            'epoch': best_epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_acc': best_acc\n        }, checkpoint_path)\n\n    # early stop\n    if val_loss < best_loss:\n        best_loss = val_loss\n        counter_loss = 0\n    else:\n        counter_loss += 1\n        if counter_loss >= patience:\n            print(f\"Early stopping triggered at {epoch + 1}\")\n            break\n\n    # save history\n    history[\"train_loss\"].append(train_loss)\n    history[\"val_loss\"].append(val_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_acc\"].append(val_acc)\n    history[\"train_f1\"].append(train_f1)\n    history[\"val_f1\"].append(val_f1)\n    history[\"train_precision\"].append(train_precision)\n    history[\"val_precision\"].append(val_precision)\n    history[\"train_recall\"].append(train_recall)\n    history[\"val_recall\"].append(val_recall)\n\n# save model last epoch\ntorch.save(model.state_dict(), '/kaggle/working/last_epoch_model.pt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = range(1, len(history[\"train_loss\"]) + 1)\n\nmetrics = {\n    \"loss\": (\"train_loss\", \"val_loss\", \"Loss\"),\n    \"accuracy\": (\"train_acc\", \"val_acc\", \"Accuracy\"),\n    \"precision\": (\"train_precision\", \"val_precision\", \"Precision\"),\n    \"recall\": (\"train_recall\", \"val_recall\", \"Recall\"),\n    \"f1\": (\"train_f1\", \"val_f1\", \"F1\"),\n}\n\n# Loop untuk bikin satu gambar per metric\nfor metric_name, (train_key, val_key, title) in metrics.items():\n    plt.figure(figsize=(8, 6))\n    plt.plot(epochs, history[train_key], label=\"Train\")\n    plt.plot(epochs, history[val_key], label=\"Validation\")\n    plt.title(f\"{title} per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(title)\n    plt.legend()\n    plt.grid(True, linestyle=\"--\", alpha=0.6)\n    plt.tight_layout()\n    plt.savefig(f\"{metric_name}_plot.png\", dpi=300)\n    plt.show()\n\n# Save history ke CSV\nwith open(\"/kaggle/working/training_history.csv\", mode=\"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow(history.keys())  # Header\n    writer.writerows(zip(*history.values()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_labels = ['B', 'C', 'I', 'T', 'S', 'E', 'G', 'H']\ntest_loss, test_acc, test_f1, test_precision, test_recall = test_model(model, test_loader, criterion, device, id2tag, unique_labels)\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test F1: {test_f1:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/working/best_checkpoint.pt\")['model_state_dict'])\ntest_loss, test_acc, test_f1, test_precision, test_recall = test_model(model, test_loader, criterion, device, id2tag, unique_labels)\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test F1: {test_f1:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/predictions_and_labels.csv\")\n\n# Mapping Q8 → Q3\nq8_to_q3 = {\n    \"E\": \"E\",\n    \"B\": \"E\",\n    \"H\": \"H\",\n    \"G\": \"H\",\n    \"I\": \"H\",\n    \"C\": \"C\",\n    \"T\": \"C\",\n    \"S\": \"C\"\n}\n\ndef convert_sequence(seq):\n    return [q8_to_q3[str(x)] for x in str(seq).split()]\n\npreds_q3 = df[\"predictions\"].apply(convert_sequence).tolist()\nlabels_q3 = df[\"labels\"].apply(convert_sequence).tolist()\n\nacc = accuracy_score(preds_q3, labels_q3)\nprecision = precision_score(preds_q3, labels_q3)\nrecall = recall_score(preds_q3, labels_q3)\nf1 = f1_score(preds_q3, labels_q3)\n\nprint(\"Accuracy:\", acc)\nprint(\"F1-score:\", f1)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\ny_pred = [x for row in preds_q3 for x in row]\ny_true = [x for row in labels_q3 for x in row]\n\ncm = confusion_matrix(y_true, y_pred, labels=[\"H\",\"E\",\"C\"])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"H\",\"E\",\"C\"])\ndisp.plot(cmap=plt.cm.Blues, xticks_rotation=45)\nplt.title(\"Confusion Matrix Q3\")\nplt.tight_layout()\nplt.show()\nplt.savefig(f\"/kaggle/working/confusion_matrix_q3.png\")\nplt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}